{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgu+fwdI41rbMUtwQvA7ml",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaglino/V-prop-SNN/blob/main/V_prop_SNN__edge_based_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewXPuplQcvmk"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Edge-list V-prop SNN for MNIST – using\n",
        "O(E) memory for spikes / voltages.\n",
        "\"\"\"\n",
        "\n",
        "# ——————————————————————————————————————————————————————————————\n",
        "# Imports\n",
        "# ——————————————————————————————————————————————————————————————\n",
        "import jax, jax.numpy as jnp\n",
        "from jax import random, jit, lax\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "import seaborn as sns, optax, time\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from skimage.transform import resize\n",
        "from keras.datasets import mnist\n",
        "\n",
        "\n",
        "# ——————————————————————————————————————————————————————————————\n",
        "# Global constants\n",
        "# ——————————————————————————————————————————————————————————————\n",
        "thresh = 0.5\n",
        "key    = random.PRNGKey(1)\n",
        "dt = ds = 1.0\n",
        "t_max  = 30\n",
        "tau    = 10.0\n",
        "α      = 0.5\n",
        "v_rest = 0.0\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 0. Original dense-implementation helpers (unchanged bodies)\n",
        "# ==============================================================\n",
        "\n",
        "def create_directed_small_world(\n",
        "    n_total=402,\n",
        "    n_inputs=196,\n",
        "    n_outputs=10,\n",
        "    k_nearest=4,\n",
        "    p_rewire=0.1,\n",
        "    p_connect=0.1\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates a directed small-world adjacency matrix with\n",
        "    – no inbound edges to inputs,\n",
        "    – no outbound edges from outputs,\n",
        "    – ring-rewired hidden layer.\n",
        "    A[i,j] == 1 ⇒ edge i → j.\n",
        "    \"\"\"\n",
        "    key = random.PRNGKey(0)\n",
        "\n",
        "    A = jnp.zeros((n_total, n_total), dtype=jnp.int32)\n",
        "\n",
        "    hidden_start = n_inputs\n",
        "    hidden_end   = n_total - n_outputs\n",
        "    n_hidden     = hidden_end - hidden_start\n",
        "    hidden_slice = slice(hidden_start, hidden_end)\n",
        "\n",
        "    # 2) regular ring\n",
        "    indices = jnp.arange(hidden_start, hidden_end)\n",
        "    for i in range(n_hidden):\n",
        "        ring_targets = (i + jnp.arange(1, k_nearest + 1)) % n_hidden + hidden_start\n",
        "        A = A.at[indices[i], ring_targets].set(1)\n",
        "\n",
        "    # 3) Watts–Strogatz rewiring\n",
        "    hidden_connections = A[hidden_slice, hidden_slice]\n",
        "    key, subkey = random.split(key)\n",
        "    rand_mat    = random.uniform(subkey, hidden_connections.shape)\n",
        "    to_rewire   = (rand_mat < p_rewire) & (hidden_connections == 1)\n",
        "    hidden_connections = hidden_connections & (~to_rewire)\n",
        "    src_rows, _ = jnp.where(to_rewire)\n",
        "    num_r       = src_rows.shape[0]\n",
        "    if num_r:\n",
        "        key, subkey = random.split(key)\n",
        "        new_cols = random.randint(subkey, (num_r,), 0, n_hidden)\n",
        "        for s, t in zip(src_rows, new_cols):\n",
        "            if s != t:\n",
        "                hidden_connections = hidden_connections.at[s, t].set(1)\n",
        "    A = A.at[hidden_slice, hidden_slice].set(hidden_connections)\n",
        "\n",
        "    # 4) extra random shortcuts\n",
        "    key, subkey = random.split(key)\n",
        "    extra = (random.uniform(subkey, (n_hidden, n_hidden)) < p_connect)\n",
        "    extra = extra & (~(hidden_connections.T == 1))\n",
        "    hidden_connections = (hidden_connections == 1) | extra\n",
        "    A = A.at[hidden_slice, hidden_slice].set(hidden_connections.astype(jnp.int32))\n",
        "\n",
        "    # 5) input → hidden\n",
        "    key, subkey = random.split(key)\n",
        "    in2hid = (random.uniform(subkey, (n_inputs, n_hidden)) < 2*p_connect)\n",
        "    A = A.at[:n_inputs, hidden_slice].set(in2hid)\n",
        "\n",
        "    # 6) hidden → output\n",
        "    key, subkey = random.split(key)\n",
        "    hid2out = (random.uniform(subkey, (n_hidden, n_outputs)) < 2*p_connect)\n",
        "    A = A.at[hidden_slice, n_total-n_outputs:].set(hid2out)\n",
        "\n",
        "    # 7) guarantee ≥1 outgoing edge per input\n",
        "    need = (jnp.sum(A[:n_inputs], 1) == 0)\n",
        "    if jnp.any(need):\n",
        "        key, subkey = random.split(key)\n",
        "        t = random.randint(subkey, (n_inputs,), hidden_start, hidden_end)\n",
        "        for i in range(n_inputs):\n",
        "            if need[i]:\n",
        "                A = A.at[i, t[i]].set(1)\n",
        "\n",
        "    # 8) guarantee ≥1 inbound edge per output\n",
        "    need = (jnp.sum(A[:, n_total-n_outputs:], 0) == 0)\n",
        "    if jnp.any(need):\n",
        "        key, subkey = random.split(key)\n",
        "        s = random.randint(subkey, (n_outputs,), hidden_start, hidden_end)\n",
        "        for i in range(n_outputs):\n",
        "            if need[i]:\n",
        "                A = A.at[s[i], n_total-n_outputs+i].set(1)\n",
        "\n",
        "    # 9) strip forbidden connections\n",
        "    A = A.at[:, :n_inputs].set(0)\n",
        "    A = A.at[-n_outputs:, :].set(0)\n",
        "\n",
        "    layers = [\n",
        "        ('Input',  (n_inputs,), 0, n_inputs-1),\n",
        "        ('Hidden', (n_hidden,), hidden_start, hidden_end-1),\n",
        "        ('Output', (n_outputs,), n_total-n_outputs, n_total-1)\n",
        "    ]\n",
        "    return A, layers\n",
        "\n",
        "\n",
        "def plot_adjacency_matrix(A):\n",
        "    plt.figure(figsize=(6,6))\n",
        "    sns.heatmap(A)\n",
        "    plt.title(\"Adjacency matrix\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def load_mnist_data(\n",
        "    train_size=0.8, val_size=0.1, test_size=0.1,\n",
        "    resample_fraction=0.2, target_size=(14,14), random_state=42\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads, normalises, downsizes and stratified-resamples MNIST.\n",
        "    Returns JAX arrays (x_train, x_val, x_test, y_train, … small labels)\n",
        "    identical to the dense helper.\n",
        "    \"\"\"\n",
        "    if not np.isclose(train_size + val_size + test_size, 1.0):\n",
        "        raise ValueError(\"Splits must sum to 1.0\")\n",
        "\n",
        "    (X_tr, y_tr), (X_te, y_te) = mnist.load_data()\n",
        "    X, y = np.concatenate([X_tr, X_te]), np.concatenate([y_tr, y_te])\n",
        "\n",
        "    X = X.astype(np.float32) / 255.0\n",
        "    X = StandardScaler().fit_transform(X.reshape(len(X), -1)).reshape(X.shape)\n",
        "\n",
        "    def _resize(imgs, size):\n",
        "        out = np.zeros((len(imgs), *size))\n",
        "        for i in range(len(imgs)):\n",
        "            out[i] = resize(imgs[i], size, anti_aliasing=True)\n",
        "        return out\n",
        "    X = _resize(X, target_size)[..., None]\n",
        "\n",
        "    n_tot = int(resample_fraction * len(X))\n",
        "    X_small, y_small = resample(X, y, n_samples=n_tot, random_state=random_state)\n",
        "\n",
        "    n_tr = int(n_tot*train_size)\n",
        "    n_v  = int(n_tot*val_size)\n",
        "    perm = np.random.RandomState(random_state).permutation(n_tot)\n",
        "    tr, v, te = perm[:n_tr], perm[n_tr:n_tr+n_v], perm[n_tr+n_v:]\n",
        "\n",
        "    x_train, x_val, x_test = map(jnp.array, (X_small[tr], X_small[v], X_small[te]))\n",
        "    y_train = jax.nn.one_hot(y_small[tr], 10)\n",
        "    y_val   = jax.nn.one_hot(y_small[v],  10)\n",
        "    y_test  = jax.nn.one_hot(y_small[te], 10)\n",
        "\n",
        "    return (x_train, x_val, x_test,\n",
        "            y_train, y_val, y_test,\n",
        "            y_small[tr], y_small[v], y_small[te])\n",
        "\n",
        "\n",
        "# Basic math helpers\n",
        "@jit\n",
        "def v(ds, dt):          return ds / dt\n",
        "@jit\n",
        "def relu_shift(x, th):  return jnp.maximum(0., x - th)\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 1. Build the graph, convert to edge lists\n",
        "# ==============================================================\n",
        "\n",
        "adj_dense, layers = create_directed_small_world(\n",
        "    n_total = 1510, n_inputs=784, n_outputs=10,\n",
        "    k_nearest=16, p_rewire=0.4, p_connect=0.1\n",
        ")\n",
        "plot_adjacency_matrix(adj_dense)\n",
        "\n",
        "N_NEURONS = adj_dense.shape[0]\n",
        "N_INPUTS  = 784\n",
        "N_OUTPUTS = 10\n",
        "\n",
        "src_np, tgt_np = np.nonzero(np.array(adj_dense))\n",
        "src = jnp.array(src_np, dtype=jnp.int32)\n",
        "tgt = jnp.array(tgt_np, dtype=jnp.int32)\n",
        "E   = len(src)\n",
        "\n",
        "vmax = v(ds, dt)\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 2. Per-edge initialisation\n",
        "# ==============================================================\n",
        "\n",
        "def initialize_lengths(adj, lengths=jnp.arange(4., 6., 1.), key=random.PRNGKey(1)):\n",
        "    key, sub = random.split(key)\n",
        "    L = adj * random.choice(sub, lengths, adj.shape)\n",
        "    return L, key\n",
        "\n",
        "def initialize_weights(adj, p_inhib=0.0, key=random.PRNGKey(2)):\n",
        "    key, sk1, sk2 = random.split(key, 3)\n",
        "    W = adj * random.uniform(sk1, adj.shape)\n",
        "    sign = (random.uniform(sk2, adj.shape) > p_inhib) * 2 - 1\n",
        "    W = W * sign * adj\n",
        "    return W, key\n",
        "\n",
        "def initialise_edges(adj, key):\n",
        "    Lmat, key = initialize_lengths(adj, jnp.arange(3., 8., ds), key)\n",
        "    L_e       = Lmat[src, tgt]\n",
        "    L_e = L_e.at[src < N_INPUTS].set(3.0)          # force pixel-edges length 3\n",
        "    Wmat, key = initialize_weights(adj, 0.0, key)\n",
        "    W_e       = Wmat[src, tgt]\n",
        "    return L_e, W_e, key\n",
        "\n",
        "L_e, W_e, key = initialise_edges(adj_dense, key)\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 3. Edge-tensor primitives (1 ↔ 1 with dense ops)\n",
        "# ==============================================================\n",
        "\n",
        "SRC_EDGES = (src < N_INPUTS)        # boolean mask, length E\n",
        "\n",
        "def scatter_edges_to_nodes(edge_vals):\n",
        "    \"\"\"(B,E) → (B,N) – sums edge currents into their *target* neurons.\"\"\"\n",
        "    return jax.vmap(lambda ev: jnp.zeros(N_NEURONS).at[tgt].add(ev))(edge_vals)\n",
        "\n",
        "@jit\n",
        "def take_step_edges(S):  return S + (S > 0) * dt * vmax\n",
        "\n",
        "# @jit\n",
        "# def check_arrival_edges(S, L):  return (S == L)          # exact int lengths\n",
        "@jit\n",
        "def check_arrival_edges(S, L_e, *, atol=1e-5, rtol=1e-8):\n",
        "    \"\"\"\n",
        "    Detect when a spike has traversed its edge length, with tolerance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    S   : jnp.ndarray, shape (batch_size, E)\n",
        "          Current spike positions (distance already travelled along each edge).\n",
        "    L_e : jnp.ndarray, shape (E,)\n",
        "          Edge lengths.\n",
        "    atol, rtol : float\n",
        "          Absolute and relative tolerances for jnp.isclose.  Defaults match\n",
        "          the dense reference code (atol=1e-5, rtol=1e-8).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    arrived : jnp.ndarray, bool, shape (batch_size, E)\n",
        "              True where the spike has reached (or very nearly reached) the\n",
        "              target node.\n",
        "    \"\"\"\n",
        "    return jnp.isclose(S, L_e, atol=atol, rtol=rtol)\n",
        "@jit\n",
        "def excite_inputs_edges(S, V, x_batch):\n",
        "    B = x_batch.shape[0]\n",
        "    pix = x_batch.reshape(B, -1)                        # (B,784)\n",
        "\n",
        "    idle_mask = (S[:, SRC_EDGES] == 0)\n",
        "    S = S.at[:, SRC_EDGES].set(jnp.where(idle_mask, dt*vmax, S[:, SRC_EDGES]))\n",
        "    V = V.at[:, SRC_EDGES].set(jnp.where(idle_mask, pix[:, src[SRC_EDGES]], V[:, SRC_EDGES]))\n",
        "    return S, V\n",
        "\n",
        "@jit\n",
        "def synapse_integration_edges(Vm, W_e, V_arr):\n",
        "    I_syn = scatter_edges_to_nodes(V_arr * W_e)\n",
        "    dV    = (-Vm + I_syn) * (dt / tau)\n",
        "    Vm    = Vm + dV\n",
        "    Y     = jnp.maximum(0., Vm - thresh)\n",
        "    return Y, Vm\n",
        "\n",
        "@jit\n",
        "def update_spikes_edges(S, V_exc, fired):\n",
        "    idle = (S == 0)\n",
        "    newS = fired[:, src] & idle\n",
        "    newV = newS * V_exc[:, src]\n",
        "    return newS, newV\n",
        "\n",
        "@jit\n",
        "def delete_finished_edges(S, V, arrived):\n",
        "    return S * (~arrived), V * (~arrived)\n",
        "\n",
        "@jit\n",
        "def reset_vm(Vm, fired):\n",
        "    return Vm - Vm*fired - 0.2*fired\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 4. **Fixed** RSNN step & inference – stores Vm *before* reset\n",
        "# ==============================================================\n",
        "\n",
        "def make_edge_step(W_e, L_e, dropout_rate):\n",
        "    @jit\n",
        "    def _step(carry, key):\n",
        "        S, V, Vm, x, acc = carry\n",
        "\n",
        "        # 1) excite inputs\n",
        "        S, V = excite_inputs_edges(S, V, x)\n",
        "\n",
        "        # 2-3) spike arrivals\n",
        "        arrived = check_arrival_edges(S, L_e)\n",
        "        V_arr   = V * arrived\n",
        "\n",
        "        # 4) synaptic integration\n",
        "        V_exc, Vm = synapse_integration_edges(Vm, W_e, V_arr)\n",
        "\n",
        "        # 5) threshold (outputs don’t fire)\n",
        "        fired = V_exc > 0\n",
        "        fired = fired.at[:, -N_OUTPUTS:].set(False)\n",
        "\n",
        "        # 6) dropout\n",
        "        keep_p = 1.0 - dropout_rate\n",
        "        mask   = random.bernoulli(key, keep_p, fired.shape)\n",
        "        fired &= mask\n",
        "        V_exc  = V_exc * mask / keep_p\n",
        "\n",
        "        # 7) spawn new spikes\n",
        "        newS, newV = update_spikes_edges(S, V_exc, fired)\n",
        "\n",
        "        # 8) delete finished spikes\n",
        "        S, V = delete_finished_edges(S, V, arrived)\n",
        "\n",
        "        # 9) **accumulate Vm BEFORE reset**\n",
        "        acc = acc + Vm                                   # running sum\n",
        "\n",
        "        # 10) reset fired neurons\n",
        "        Vm = reset_vm(Vm, fired)\n",
        "\n",
        "        # 11) advance spikes & add newborns\n",
        "        S = take_step_edges(S) + newS * dt * vmax\n",
        "        V = V + newV\n",
        "\n",
        "        return (S, V, Vm, x, acc), None\n",
        "    return _step\n",
        "\n",
        "\n",
        "@jit\n",
        "def RSNN_inference_edge(W_e, L_e, x_batch, rng_key, dropout_rate):\n",
        "    B   = x_batch.shape[0]\n",
        "    S   = V = jnp.zeros((B, E))\n",
        "    Vm  = jnp.zeros((B, N_NEURONS))\n",
        "    acc = jnp.zeros((B, N_NEURONS))          # accumulator for Vm\n",
        "\n",
        "    (S, V, Vm, x, acc), _ = lax.scan(\n",
        "        make_edge_step(W_e, L_e, dropout_rate),\n",
        "        (S, V, Vm, x_batch, acc),\n",
        "        random.split(rng_key, t_max)\n",
        "    )\n",
        "    return acc / t_max\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 5. Loss / prediction wrappers\n",
        "# ==============================================================\n",
        "\n",
        "@jit\n",
        "def loss_edge(W_e, L_e, xb, yb, key, drop):\n",
        "    Vm_mean = RSNN_inference_edge(W_e, L_e, xb, key, drop)    # (B, N)\n",
        "    logits  = Vm_mean[:, -N_OUTPUTS:]                       # (B, 10)\n",
        "    probs   = jax.nn.softmax(logits, 1)\n",
        "    return -jnp.mean(jnp.sum(yb * jnp.log(probs + 1e-8), 1))\n",
        "\n",
        "\n",
        "@jit\n",
        "def predict_edge(W_e, L_e, xb):\n",
        "    Vm_mean = RSNN_inference_edge(W_e, L_e, xb, key, 0.0)\n",
        "    logits  = Vm_mean[:, -N_OUTPUTS:]\n",
        "    return jnp.argmax(logits, 1)\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 6. Optimiser\n",
        "# ==============================================================\n",
        "\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(5.0),\n",
        "    optax.adam(0.005)\n",
        ")\n",
        "\n",
        "opt_state = optimizer.init(W_e)\n",
        "\n",
        "@jit\n",
        "def train_edge_step(W_e, L_e, opt_state, xb, yb, key):\n",
        "    loss, grads = jax.value_and_grad(loss_edge)(\n",
        "        W_e, L_e, xb, yb, key, 0.2)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, W_e)\n",
        "    W_e = optax.apply_updates(W_e, updates)\n",
        "    return W_e, opt_state, loss\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 7. Data\n",
        "# ==============================================================\n",
        "\n",
        "(x_train, x_val, x_test,\n",
        " y_train, y_val, y_test,\n",
        " y_train_lbl, y_val_lbl, y_test_lbl) = load_mnist_data(\n",
        "     train_size=0.8, val_size=0.1, test_size=0.1,\n",
        "     resample_fraction=1.0, target_size=(28,28), random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 8. Training loop\n",
        "# ==============================================================\n",
        "\n",
        "batch_size = 32\n",
        "epochs     = 20\n",
        "rng = random.PRNGKey(42)\n",
        "\n",
        "\n",
        "print(\"Starting training…\"); t0 = time.time()\n",
        "for ep in range(epochs):\n",
        "    # --- shuffle training data ------------------------------------------------\n",
        "    perm = np.random.permutation(len(x_train))\n",
        "    x_tr, y_tr = x_train[perm], y_train[perm]\n",
        "\n",
        "    # --- TRAIN -----------------------------------------------------------------\n",
        "    train_loss, nb = 0.0, 0\n",
        "    rng, sk = random.split(rng)\n",
        "    for i in range(0, len(x_train) - batch_size + 1, batch_size):\n",
        "        xb, yb = x_tr[i:i+batch_size], y_tr[i:i+batch_size]\n",
        "        sk, sub = random.split(sk)\n",
        "        W_e, opt_state, lv = train_edge_step(W_e, L_e, opt_state, xb, yb, sub)\n",
        "        train_loss += float(lv);  nb += 1\n",
        "    train_loss /= nb\n",
        "\n",
        "    # --- VALIDATE (no dropout) -------------------------------------------------\n",
        "    val_loss, nbv = 0.0, 0\n",
        "    preds = []\n",
        "    for j in range(0, len(x_val) - batch_size + 1, batch_size):\n",
        "        xb, yb = x_val[j:j+batch_size], y_val[j:j+batch_size]\n",
        "        rng, sub = random.split(rng)\n",
        "        val_loss += float(loss_edge(W_e, L_e, xb, yb, sub, 0.0));  nbv += 1\n",
        "        preds.append(predict_edge(W_e, L_e, xb))\n",
        "    val_loss /= nbv\n",
        "\n",
        "    preds = jnp.concatenate(preds)\n",
        "    val_acc = float((preds == y_val_lbl[:len(preds)]).mean()) * 100.0\n",
        "\n",
        "    # --- LOG -------------------------------------------------------------------\n",
        "    print(f\"Epoch {ep+1:02d} | \"\n",
        "          f\"train loss {train_loss:.4f} | \"\n",
        "          f\"val loss {val_loss:.4f} | \"\n",
        "          f\"val acc {val_acc:.2f} %\")\n",
        "\n",
        "print(\"Training finished in\", round(time.time() - t0, 1), \"s\")\n",
        "\n",
        "# ==============================================================\n",
        "# 9. Test accuracy\n",
        "# ==============================================================\n",
        "\n",
        "pred=[]\n",
        "for i in range(0, len(x_test)-batch_size+1, batch_size):\n",
        "    pred.append(predict_edge(W_e, L_e, x_test[i:i+batch_size]))\n",
        "pred = jnp.concatenate(pred)\n",
        "acc  = (pred == y_test_lbl[:len(pred)]).mean() * 100\n",
        "print(f\"Final test accuracy: {acc:.2f} %\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Edge-list SNN – bug-fixed and self-contained\")"
      ]
    }
  ]
}